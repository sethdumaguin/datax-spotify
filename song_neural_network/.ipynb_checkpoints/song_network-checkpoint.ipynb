{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv('songz.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we have mapped 0 to relaxed, 1 to angry, 2 to happy, and 3 to sad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(train, test):\n",
    "\n",
    "    mean_train = np.mean(train, axis=0)\n",
    "    std_train = np.std(train, axis=0)+0.000001\n",
    "    mean_test = np.mean(test, axis=0)\n",
    "    std_test = np.std(test, axis=0)+0.000001\n",
    "    X_train = (train - mean_train) / std_train\n",
    "    X_test = (test - mean_test) /std_test\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X = d.loc[:, ['title','artist','val','dB','bpm', 'nrgy', 'dnce']]\n",
    "y = d[['mood']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "le = LabelEncoder()\n",
    "X.loc[:,'title'] = le.fit_transform(X.loc[:,'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "le2 = LabelEncoder()\n",
    "X.loc[:,'artist'] = le2.fit_transform(X.loc[:,'artist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X['title'] =([int(a) for a in ['title']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "X, y = shuffle(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "X_test = X[1200:]\n",
    "y_test = y[1200:]\n",
    "y_final = y[1200:]\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "y_test = scaler.fit_transform(y_test)\n",
    "# X_test,_ = standardize(X_test, [0])\n",
    "# y_test, _ = standardize(y_test, [0])\n",
    "from keras.utils import np_utils\n",
    "from keras.utils import normalize\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_test)\n",
    "encoded_Y = encoder.transform(y_test)\n",
    "y_test = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[0:900]\n",
    "Y_train = y[0:900]\n",
    "X_val = X[900:1200]\n",
    "Y_val = y[900:1200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "# X_train, X_val = standardize(X_train, X_val)\n",
    "# Y_train, Y_val = standardize(Y_train,Y_val)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.fit_transform(X_val)\n",
    "Y_train= scaler.fit_transform(Y_train)\n",
    "Y_val = scaler.fit_transform(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.utils import normalize\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y_train)\n",
    "encoded_Y = encoder.transform(Y_train)\n",
    "\n",
    "Y_train =  np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.utils import normalize\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y_val)\n",
    "encoded_Y = encoder.transform(Y_val)\n",
    "Y_val = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 48)                384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 48)                192       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 24)                1176      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 24)                96        \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 24)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 12)                300       \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 12)                48        \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 78        \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 6)                 24        \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 4)                 28        \n",
      "=================================================================\n",
      "Total params: 2,326\n",
      "Trainable params: 2,146\n",
      "Non-trainable params: 180\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import BatchNormalization\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# model.add(Dense(64, input_shape=(7,), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(LeakyReLU(alpha=0.001))\n",
    "# model.add(Dropout(0.4))\n",
    "model.add(Dense(48, activation='relu', input_shape=(7,)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(24))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(12))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(6))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# #model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(Dense(4, activation='sigmoid', name='output'))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 900 samples, validate on 300 samples\n",
      "Epoch 1/50\n",
      " - 2s - loss: 0.8114 - acc: 0.5069 - val_loss: 0.7555 - val_acc: 0.5033\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.50333, saving model to best_model.h5\n",
      "Epoch 2/50\n",
      " - 0s - loss: 0.7850 - acc: 0.5131 - val_loss: 0.7355 - val_acc: 0.5450\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.50333 to 0.54500, saving model to best_model.h5\n",
      "Epoch 3/50\n",
      " - 0s - loss: 0.7569 - acc: 0.5278 - val_loss: 0.7124 - val_acc: 0.5650\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.54500 to 0.56500, saving model to best_model.h5\n",
      "Epoch 4/50\n",
      " - 0s - loss: 0.7577 - acc: 0.5292 - val_loss: 0.6937 - val_acc: 0.5875\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.56500 to 0.58750, saving model to best_model.h5\n",
      "Epoch 5/50\n",
      " - 0s - loss: 0.7408 - acc: 0.5444 - val_loss: 0.6866 - val_acc: 0.5983\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.58750 to 0.59833, saving model to best_model.h5\n",
      "Epoch 6/50\n",
      " - 0s - loss: 0.7369 - acc: 0.5492 - val_loss: 0.6845 - val_acc: 0.6125\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.59833 to 0.61250, saving model to best_model.h5\n",
      "Epoch 7/50\n",
      " - 0s - loss: 0.7313 - acc: 0.5461 - val_loss: 0.6799 - val_acc: 0.6167\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.61250 to 0.61667, saving model to best_model.h5\n",
      "Epoch 8/50\n",
      " - 0s - loss: 0.7215 - acc: 0.5617 - val_loss: 0.6740 - val_acc: 0.6333\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.61667 to 0.63333, saving model to best_model.h5\n",
      "Epoch 9/50\n",
      " - 0s - loss: 0.7139 - acc: 0.5678 - val_loss: 0.6718 - val_acc: 0.6342\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.63333 to 0.63417, saving model to best_model.h5\n",
      "Epoch 10/50\n",
      " - 0s - loss: 0.7068 - acc: 0.5764 - val_loss: 0.6685 - val_acc: 0.6417\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.63417 to 0.64167, saving model to best_model.h5\n",
      "Epoch 11/50\n",
      " - 0s - loss: 0.7087 - acc: 0.5731 - val_loss: 0.6675 - val_acc: 0.6500\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.64167 to 0.65000, saving model to best_model.h5\n",
      "Epoch 12/50\n",
      " - 0s - loss: 0.6922 - acc: 0.5883 - val_loss: 0.6655 - val_acc: 0.6575\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.65000 to 0.65750, saving model to best_model.h5\n",
      "Epoch 13/50\n",
      " - 0s - loss: 0.6934 - acc: 0.5867 - val_loss: 0.6639 - val_acc: 0.6683\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.65750 to 0.66833, saving model to best_model.h5\n",
      "Epoch 14/50\n",
      " - 0s - loss: 0.6875 - acc: 0.5989 - val_loss: 0.6624 - val_acc: 0.6800\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.66833 to 0.68000, saving model to best_model.h5\n",
      "Epoch 15/50\n",
      " - 0s - loss: 0.6811 - acc: 0.6058 - val_loss: 0.6593 - val_acc: 0.6850\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.68000 to 0.68500, saving model to best_model.h5\n",
      "Epoch 16/50\n",
      " - 0s - loss: 0.6747 - acc: 0.6144 - val_loss: 0.6558 - val_acc: 0.6833\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.68500\n",
      "Epoch 17/50\n",
      " - 0s - loss: 0.6663 - acc: 0.6303 - val_loss: 0.6520 - val_acc: 0.6850\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.68500 to 0.68500, saving model to best_model.h5\n",
      "Epoch 18/50\n",
      " - 0s - loss: 0.6681 - acc: 0.6158 - val_loss: 0.6493 - val_acc: 0.6883\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.68500 to 0.68833, saving model to best_model.h5\n",
      "Epoch 19/50\n",
      " - 0s - loss: 0.6613 - acc: 0.6258 - val_loss: 0.6470 - val_acc: 0.6917\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.68833 to 0.69167, saving model to best_model.h5\n",
      "Epoch 20/50\n",
      " - 0s - loss: 0.6606 - acc: 0.6369 - val_loss: 0.6435 - val_acc: 0.6925\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.69167 to 0.69250, saving model to best_model.h5\n",
      "Epoch 21/50\n",
      " - 0s - loss: 0.6496 - acc: 0.6431 - val_loss: 0.6395 - val_acc: 0.6983\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.69250 to 0.69833, saving model to best_model.h5\n",
      "Epoch 22/50\n",
      " - 0s - loss: 0.6498 - acc: 0.6464 - val_loss: 0.6359 - val_acc: 0.6983\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.69833\n",
      "Epoch 23/50\n",
      " - 0s - loss: 0.6473 - acc: 0.6550 - val_loss: 0.6317 - val_acc: 0.6983\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.69833\n",
      "Epoch 24/50\n",
      " - 0s - loss: 0.6447 - acc: 0.6592 - val_loss: 0.6277 - val_acc: 0.7042\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.69833 to 0.70417, saving model to best_model.h5\n",
      "Epoch 25/50\n",
      " - 0s - loss: 0.6385 - acc: 0.6589 - val_loss: 0.6237 - val_acc: 0.7075\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.70417 to 0.70750, saving model to best_model.h5\n",
      "Epoch 26/50\n",
      " - 0s - loss: 0.6321 - acc: 0.6719 - val_loss: 0.6219 - val_acc: 0.7008\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.70750\n",
      "Epoch 27/50\n",
      " - 0s - loss: 0.6307 - acc: 0.6742 - val_loss: 0.6192 - val_acc: 0.7042\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.70750\n",
      "Epoch 28/50\n",
      " - 0s - loss: 0.6304 - acc: 0.6747 - val_loss: 0.6164 - val_acc: 0.7058\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.70750\n",
      "Epoch 29/50\n",
      " - 0s - loss: 0.6307 - acc: 0.6872 - val_loss: 0.6132 - val_acc: 0.7108\n",
      "\n",
      "Epoch 00029: val_acc improved from 0.70750 to 0.71083, saving model to best_model.h5\n",
      "Epoch 30/50\n",
      " - 0s - loss: 0.6232 - acc: 0.6800 - val_loss: 0.6100 - val_acc: 0.7150\n",
      "\n",
      "Epoch 00030: val_acc improved from 0.71083 to 0.71500, saving model to best_model.h5\n",
      "Epoch 31/50\n",
      " - 0s - loss: 0.6278 - acc: 0.6772 - val_loss: 0.6073 - val_acc: 0.7167\n",
      "\n",
      "Epoch 00031: val_acc improved from 0.71500 to 0.71667, saving model to best_model.h5\n",
      "Epoch 32/50\n",
      " - 0s - loss: 0.6209 - acc: 0.6811 - val_loss: 0.6045 - val_acc: 0.7208\n",
      "\n",
      "Epoch 00032: val_acc improved from 0.71667 to 0.72083, saving model to best_model.h5\n",
      "Epoch 33/50\n",
      " - 0s - loss: 0.6182 - acc: 0.6939 - val_loss: 0.6023 - val_acc: 0.7192\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.72083\n",
      "Epoch 34/50\n",
      " - 0s - loss: 0.6193 - acc: 0.6867 - val_loss: 0.6011 - val_acc: 0.7133\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.72083\n",
      "Epoch 35/50\n",
      " - 0s - loss: 0.6178 - acc: 0.6983 - val_loss: 0.5997 - val_acc: 0.7150\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.72083\n",
      "Epoch 36/50\n",
      " - 0s - loss: 0.6113 - acc: 0.7006 - val_loss: 0.5983 - val_acc: 0.7200\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.72083\n",
      "Epoch 37/50\n",
      " - 0s - loss: 0.6089 - acc: 0.6958 - val_loss: 0.5966 - val_acc: 0.7217\n",
      "\n",
      "Epoch 00037: val_acc improved from 0.72083 to 0.72167, saving model to best_model.h5\n",
      "Epoch 38/50\n",
      " - 0s - loss: 0.6117 - acc: 0.6956 - val_loss: 0.5949 - val_acc: 0.7225\n",
      "\n",
      "Epoch 00038: val_acc improved from 0.72167 to 0.72250, saving model to best_model.h5\n",
      "Epoch 39/50\n",
      " - 0s - loss: 0.6043 - acc: 0.7000 - val_loss: 0.5927 - val_acc: 0.7242\n",
      "\n",
      "Epoch 00039: val_acc improved from 0.72250 to 0.72417, saving model to best_model.h5\n",
      "Epoch 40/50\n",
      " - 0s - loss: 0.6045 - acc: 0.7031 - val_loss: 0.5898 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00040: val_acc improved from 0.72417 to 0.72917, saving model to best_model.h5\n",
      "Epoch 41/50\n",
      " - 0s - loss: 0.6054 - acc: 0.7028 - val_loss: 0.5874 - val_acc: 0.7325\n",
      "\n",
      "Epoch 00041: val_acc improved from 0.72917 to 0.73250, saving model to best_model.h5\n",
      "Epoch 42/50\n",
      " - 0s - loss: 0.6002 - acc: 0.7056 - val_loss: 0.5857 - val_acc: 0.7325\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.73250\n",
      "Epoch 43/50\n",
      " - 0s - loss: 0.5958 - acc: 0.7108 - val_loss: 0.5845 - val_acc: 0.7350\n",
      "\n",
      "Epoch 00043: val_acc improved from 0.73250 to 0.73500, saving model to best_model.h5\n",
      "Epoch 44/50\n",
      " - 0s - loss: 0.5970 - acc: 0.7142 - val_loss: 0.5831 - val_acc: 0.7367\n",
      "\n",
      "Epoch 00044: val_acc improved from 0.73500 to 0.73667, saving model to best_model.h5\n",
      "Epoch 45/50\n",
      " - 0s - loss: 0.5989 - acc: 0.7081 - val_loss: 0.5818 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.73667\n",
      "Epoch 46/50\n",
      " - 0s - loss: 0.5900 - acc: 0.7167 - val_loss: 0.5803 - val_acc: 0.7350\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.73667\n",
      "Epoch 47/50\n",
      " - 0s - loss: 0.5908 - acc: 0.7150 - val_loss: 0.5787 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.73667\n",
      "Epoch 48/50\n",
      " - 0s - loss: 0.5914 - acc: 0.7172 - val_loss: 0.5772 - val_acc: 0.7350\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.73667\n",
      "Epoch 49/50\n",
      " - 0s - loss: 0.5887 - acc: 0.7158 - val_loss: 0.5758 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.73667\n",
      "Epoch 50/50\n",
      " - 0s - loss: 0.5878 - acc: 0.7156 - val_loss: 0.5743 - val_acc: 0.7333\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.73667\n"
     ]
    }
   ],
   "source": [
    "# Adam optimizer with learning rate of 0.001\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "optimizer = Adam(lr=0.001)\n",
    "adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-10, decay=0)\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(optimizer = 'adam' , loss='binary_crossentropy', metrics=['accuracy'])\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "hist = model.fit(X_train, Y_train, verbose=2, batch_size=128, epochs=50, validation_data=(X_val,Y_val),callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5716227218015304, 0.7400662259550284]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(1, figsize=(10,7))\n",
    "\n",
    "# Training accuracy\n",
    "plt.subplot(221)\n",
    "plt.plot(range(0,50), hist.history['acc'], c = 'forestgreen');\n",
    "plt.title('Training accuracy per epoch');\n",
    "\n",
    "# Training loss #\n",
    "plt.subplot(222)\n",
    "plt.plot(range(0,50), hist.history['loss'], c = 'forestgreen');\n",
    "plt.title('Training loss per epoch');\n",
    "\n",
    "# Validation accuracy #\n",
    "plt.subplot(223)\n",
    "plt.plot(range(0,50), hist.history['val_acc'], c = 'blue');\n",
    "plt.title('Validation accuracy per epoch');\n",
    "\n",
    "# Validation loss #\n",
    "plt.subplot(224)\n",
    "plt.plot(range(0,50), hist.history['val_loss'], c = 'blue');\n",
    "plt.title('Validation loss per epoch');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'song_NN.pickle'\n",
    "\n",
    "pickle_out = open(filename, 'wb')\n",
    "\n",
    "pickle.dump(model, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"song_NN.pickle\",\"rb\")\n",
    "example_dict = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = example_dict.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_pred.argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
