{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv('songz.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDF = pd.DataFrame(data = {'mood': ['relaxed', 'angry', 'happy', 'sad'], 'mood score': [0,1,2,3]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mood</th>\n",
       "      <th>mood score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relaxed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>angry</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>happy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sad</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mood  mood score\n",
       "0  relaxed           0\n",
       "1    angry           1\n",
       "2    happy           2\n",
       "3      sad           3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we have mapped 0 to relaxed, 1 to angry, 2 to happy, and 3 to sad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X = d.loc[:, ['val','dB','bpm', 'nrgy', 'dnce']]\n",
    "y = d[['mood']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "# le = LabelEncoder()\n",
    "# X.loc[:,'title'] = le.fit_transform(X.loc[:,'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "# le2 = LabelEncoder()\n",
    "# X.loc[:,'artist'] = le2.fit_transform(X.loc[:,'artist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "X, y = shuffle(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "Using TensorFlow backend.\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "X_test = X[1200:]\n",
    "y_test = y[1200:]\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "y_test = scaler.fit_transform(y_test)\n",
    "# X_test,_ = standardize(X_test, [0])\n",
    "# y_test, _ = standardize(y_test, [0])\n",
    "from keras.utils import np_utils\n",
    "from keras.utils import normalize\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_test)\n",
    "encoded_Y = encoder.transform(y_test)\n",
    "y_test = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.92636858, 0.63893823, 0.48170571, 0.80598487, 0.62132353],\n",
       "       [0.98826166, 0.74773041, 0.43981796, 0.91340568, 0.7622549 ],\n",
       "       [0.14950379, 0.53952043, 0.09451241, 0.06390442, 0.41053922],\n",
       "       ...,\n",
       "       [0.17084623, 0.80229919, 0.74623516, 0.68431437, 0.33946078],\n",
       "       [0.17191335, 0.27836984, 0.40950527, 0.03321276, 0.64215686],\n",
       "       [0.87728097, 0.36495954, 0.45766199, 0.35437904, 0.3872549 ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[0:900]\n",
    "Y_train = y[0:900]\n",
    "X_val = X[900:1200]\n",
    "Y_val = y[900:1200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.fit_transform(X_val)\n",
    "Y_train= scaler.fit_transform(Y_train)\n",
    "Y_val = scaler.fit_transform(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.utils import normalize\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y_train)\n",
    "encoded_Y = encoder.transform(Y_train)\n",
    "\n",
    "Y_train =  np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.utils import normalize\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y_val)\n",
    "encoded_Y = encoder.transform(Y_val)\n",
    "Y_val = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 48)                288       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 48)                192       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 4)                 196       \n",
      "=================================================================\n",
      "Total params: 676\n",
      "Trainable params: 580\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import BatchNormalization\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# model.add(Dense(64, input_shape=(7,), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(LeakyReLU(alpha=0.001))\n",
    "# model.add(Dropout(0.4))\n",
    "model.add(Dense(48, activation='relu', input_shape=(5,)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Dense(24))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Dense(12))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Dense(6))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.4))\n",
    "\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# #model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(Dense(4, activation='sigmoid', name='output'))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 900 samples, validate on 300 samples\n",
      "Epoch 1/50\n",
      " - 1s - loss: 0.8456 - acc: 0.4969 - val_loss: 0.7592 - val_acc: 0.5333\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.53333, saving model to best_model.h5\n",
      "Epoch 2/50\n",
      " - 0s - loss: 0.8006 - acc: 0.5011 - val_loss: 0.7041 - val_acc: 0.5858\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.53333 to 0.58583, saving model to best_model.h5\n",
      "Epoch 3/50\n",
      " - 0s - loss: 0.7652 - acc: 0.5283 - val_loss: 0.6665 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.58583 to 0.62500, saving model to best_model.h5\n",
      "Epoch 4/50\n",
      " - 0s - loss: 0.7343 - acc: 0.5317 - val_loss: 0.6448 - val_acc: 0.6517\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.62500 to 0.65167, saving model to best_model.h5\n",
      "Epoch 5/50\n",
      " - 0s - loss: 0.7095 - acc: 0.5631 - val_loss: 0.6335 - val_acc: 0.6567\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.65167 to 0.65667, saving model to best_model.h5\n",
      "Epoch 6/50\n",
      " - 0s - loss: 0.6894 - acc: 0.5769 - val_loss: 0.6272 - val_acc: 0.6642\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.65667 to 0.66417, saving model to best_model.h5\n",
      "Epoch 7/50\n",
      " - 0s - loss: 0.6716 - acc: 0.6036 - val_loss: 0.6203 - val_acc: 0.6650\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.66417 to 0.66500, saving model to best_model.h5\n",
      "Epoch 8/50\n",
      " - 0s - loss: 0.6555 - acc: 0.6283 - val_loss: 0.6144 - val_acc: 0.6775\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.66500 to 0.67750, saving model to best_model.h5\n",
      "Epoch 9/50\n",
      " - 0s - loss: 0.6428 - acc: 0.6336 - val_loss: 0.6059 - val_acc: 0.6883\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.67750 to 0.68833, saving model to best_model.h5\n",
      "Epoch 10/50\n",
      " - 0s - loss: 0.6349 - acc: 0.6528 - val_loss: 0.6017 - val_acc: 0.6975\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.68833 to 0.69750, saving model to best_model.h5\n",
      "Epoch 11/50\n",
      " - 0s - loss: 0.6175 - acc: 0.6708 - val_loss: 0.5936 - val_acc: 0.7075\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.69750 to 0.70750, saving model to best_model.h5\n",
      "Epoch 12/50\n",
      " - 0s - loss: 0.6157 - acc: 0.6783 - val_loss: 0.5918 - val_acc: 0.7033\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.70750\n",
      "Epoch 13/50\n",
      " - 0s - loss: 0.6069 - acc: 0.6864 - val_loss: 0.5881 - val_acc: 0.7067\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.70750\n",
      "Epoch 14/50\n",
      " - 0s - loss: 0.6002 - acc: 0.6989 - val_loss: 0.5824 - val_acc: 0.7100\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.70750 to 0.71000, saving model to best_model.h5\n",
      "Epoch 15/50\n",
      " - 0s - loss: 0.5905 - acc: 0.7106 - val_loss: 0.5797 - val_acc: 0.7133\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.71000 to 0.71333, saving model to best_model.h5\n",
      "Epoch 16/50\n",
      " - 0s - loss: 0.5829 - acc: 0.7147 - val_loss: 0.5746 - val_acc: 0.7200\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.71333 to 0.72000, saving model to best_model.h5\n",
      "Epoch 17/50\n",
      " - 0s - loss: 0.5860 - acc: 0.7089 - val_loss: 0.5710 - val_acc: 0.7258\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.72000 to 0.72583, saving model to best_model.h5\n",
      "Epoch 18/50\n",
      " - 0s - loss: 0.5750 - acc: 0.7225 - val_loss: 0.5667 - val_acc: 0.7317\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.72583 to 0.73167, saving model to best_model.h5\n",
      "Epoch 19/50\n",
      " - 0s - loss: 0.5716 - acc: 0.7319 - val_loss: 0.5631 - val_acc: 0.7342\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.73167 to 0.73417, saving model to best_model.h5\n",
      "Epoch 20/50\n",
      " - 0s - loss: 0.5681 - acc: 0.7306 - val_loss: 0.5576 - val_acc: 0.7458\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.73417 to 0.74583, saving model to best_model.h5\n",
      "Epoch 21/50\n",
      " - 0s - loss: 0.5666 - acc: 0.7356 - val_loss: 0.5541 - val_acc: 0.7542\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.74583 to 0.75417, saving model to best_model.h5\n",
      "Epoch 22/50\n",
      " - 0s - loss: 0.5593 - acc: 0.7394 - val_loss: 0.5543 - val_acc: 0.7508\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.75417\n",
      "Epoch 23/50\n",
      " - 0s - loss: 0.5617 - acc: 0.7314 - val_loss: 0.5551 - val_acc: 0.7508\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.75417\n",
      "Epoch 24/50\n",
      " - 0s - loss: 0.5605 - acc: 0.7378 - val_loss: 0.5550 - val_acc: 0.7517\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.75417\n",
      "Epoch 25/50\n",
      " - 0s - loss: 0.5599 - acc: 0.7433 - val_loss: 0.5550 - val_acc: 0.7517\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.75417\n",
      "Epoch 26/50\n",
      " - 0s - loss: 0.5609 - acc: 0.7397 - val_loss: 0.5536 - val_acc: 0.7517\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.75417\n",
      "Epoch 27/50\n",
      " - 0s - loss: 0.5548 - acc: 0.7450 - val_loss: 0.5553 - val_acc: 0.7483\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.75417\n",
      "Epoch 28/50\n",
      " - 0s - loss: 0.5557 - acc: 0.7414 - val_loss: 0.5559 - val_acc: 0.7483\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.75417\n",
      "Epoch 29/50\n",
      " - 0s - loss: 0.5553 - acc: 0.7383 - val_loss: 0.5544 - val_acc: 0.7542\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.75417\n",
      "Epoch 30/50\n",
      " - 0s - loss: 0.5573 - acc: 0.7411 - val_loss: 0.5527 - val_acc: 0.7517\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.75417\n",
      "Epoch 31/50\n",
      " - 0s - loss: 0.5525 - acc: 0.7458 - val_loss: 0.5513 - val_acc: 0.7533\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.75417\n",
      "Epoch 32/50\n",
      " - 0s - loss: 0.5494 - acc: 0.7436 - val_loss: 0.5490 - val_acc: 0.7542\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.75417\n",
      "Epoch 33/50\n",
      " - 0s - loss: 0.5579 - acc: 0.7447 - val_loss: 0.5483 - val_acc: 0.7542\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.75417\n",
      "Epoch 34/50\n",
      " - 0s - loss: 0.5529 - acc: 0.7419 - val_loss: 0.5498 - val_acc: 0.7542\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.75417\n",
      "Epoch 35/50\n",
      " - 0s - loss: 0.5496 - acc: 0.7472 - val_loss: 0.5542 - val_acc: 0.7492\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.75417\n",
      "Epoch 36/50\n",
      " - 0s - loss: 0.5485 - acc: 0.7419 - val_loss: 0.5543 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.75417\n",
      "Epoch 37/50\n",
      " - 0s - loss: 0.5543 - acc: 0.7411 - val_loss: 0.5547 - val_acc: 0.7508\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.75417\n",
      "Epoch 38/50\n",
      " - 0s - loss: 0.5521 - acc: 0.7439 - val_loss: 0.5538 - val_acc: 0.7517\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.75417\n",
      "Epoch 39/50\n",
      " - 0s - loss: 0.5476 - acc: 0.7444 - val_loss: 0.5534 - val_acc: 0.7550\n",
      "\n",
      "Epoch 00039: val_acc improved from 0.75417 to 0.75500, saving model to best_model.h5\n",
      "Epoch 40/50\n",
      " - 0s - loss: 0.5490 - acc: 0.7481 - val_loss: 0.5515 - val_acc: 0.7542\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.75500\n",
      "Epoch 41/50\n",
      " - 0s - loss: 0.5556 - acc: 0.7411 - val_loss: 0.5497 - val_acc: 0.7542\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.75500\n",
      "Epoch 42/50\n",
      " - 0s - loss: 0.5540 - acc: 0.7442 - val_loss: 0.5484 - val_acc: 0.7550\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.75500\n",
      "Epoch 43/50\n",
      " - 0s - loss: 0.5516 - acc: 0.7431 - val_loss: 0.5473 - val_acc: 0.7550\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.75500\n",
      "Epoch 44/50\n",
      " - 0s - loss: 0.5529 - acc: 0.7436 - val_loss: 0.5490 - val_acc: 0.7567\n",
      "\n",
      "Epoch 00044: val_acc improved from 0.75500 to 0.75667, saving model to best_model.h5\n",
      "Epoch 45/50\n",
      " - 0s - loss: 0.5532 - acc: 0.7397 - val_loss: 0.5483 - val_acc: 0.7550\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.75667\n",
      "Epoch 46/50\n",
      " - 0s - loss: 0.5506 - acc: 0.7458 - val_loss: 0.5478 - val_acc: 0.7592\n",
      "\n",
      "Epoch 00046: val_acc improved from 0.75667 to 0.75917, saving model to best_model.h5\n",
      "Epoch 47/50\n",
      " - 0s - loss: 0.5485 - acc: 0.7428 - val_loss: 0.5487 - val_acc: 0.7558\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.75917\n",
      "Epoch 48/50\n",
      " - 0s - loss: 0.5494 - acc: 0.7456 - val_loss: 0.5462 - val_acc: 0.7567\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.75917\n",
      "Epoch 49/50\n",
      " - 0s - loss: 0.5526 - acc: 0.7447 - val_loss: 0.5447 - val_acc: 0.7575\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.75917\n",
      "Epoch 50/50\n",
      " - 0s - loss: 0.5472 - acc: 0.7403 - val_loss: 0.5453 - val_acc: 0.7575\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.75917\n"
     ]
    }
   ],
   "source": [
    "# Adam optimizer with learning rate of 0.001\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "optimizer = Adam(lr=0.001)\n",
    "adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-10, decay=0)\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(optimizer = 'adam' , loss='binary_crossentropy', metrics=['accuracy'])\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "hist = model.fit(X_train, Y_train, verbose=2, batch_size=128, epochs=50, validation_data=(X_val,Y_val),callbacks=[mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5361460664414412, 0.7508278145695364]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(1, figsize=(10,7))\n",
    "\n",
    "# Training accuracy\n",
    "plt.subplot(221)\n",
    "plt.plot(range(0,50), hist.history['acc'], c = 'forestgreen');\n",
    "plt.title('Training accuracy per epoch');\n",
    "\n",
    "# Training loss #\n",
    "plt.subplot(222)\n",
    "plt.plot(range(0,50), hist.history['loss'], c = 'forestgreen');\n",
    "plt.title('Training loss per epoch');\n",
    "\n",
    "# Validation accuracy #\n",
    "plt.subplot(223)\n",
    "plt.plot(range(0,50), hist.history['val_acc'], c = 'blue');\n",
    "plt.title('Validation accuracy per epoch');\n",
    "\n",
    "# Validation loss #\n",
    "plt.subplot(224)\n",
    "plt.plot(range(0,50), hist.history['val_loss'], c = 'blue');\n",
    "plt.title('Validation loss per epoch');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'song_NN.pickle'\n",
    "\n",
    "pickle_out = open(filename, 'wb')\n",
    "\n",
    "pickle.dump(model, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"song_NN.pickle\",\"rb\")\n",
    "example_dict = pickle.load(pickle_in)\n",
    "y_pred = example_dict.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.92636858, 0.63893823, 0.48170571, 0.80598487, 0.62132353],\n",
       "       [0.98826166, 0.74773041, 0.43981796, 0.91340568, 0.7622549 ],\n",
       "       [0.14950379, 0.53952043, 0.09451241, 0.06390442, 0.41053922],\n",
       "       ...,\n",
       "       [0.17084623, 0.80229919, 0.74623516, 0.68431437, 0.33946078],\n",
       "       [0.17191335, 0.27836984, 0.40950527, 0.03321276, 0.64215686],\n",
       "       [0.87728097, 0.36495954, 0.45766199, 0.35437904, 0.3872549 ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.21431434, 0.20859206, 0.31698182, 0.2460368 ],\n",
       "       [0.18490916, 0.20987427, 0.31353402, 0.30317605],\n",
       "       [0.38156712, 0.02891389, 0.688908  , 0.2196776 ],\n",
       "       ...,\n",
       "       [0.16889727, 0.33741376, 0.25874868, 0.27489048],\n",
       "       [0.09442896, 0.03617036, 0.29641145, 0.24646467],\n",
       "       [0.37267974, 0.0679768 , 0.40493056, 0.13001296]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 2, 0, 2, 2, 1, 1, 0, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2,\n",
       "       2, 2, 3, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 1, 0, 2, 0, 2, 2, 2, 1,\n",
       "       2, 2, 3, 1, 2, 2, 0, 0, 2, 1, 2, 1, 2, 2, 2, 0, 1, 2, 1, 2, 1, 0,\n",
       "       2, 0, 2, 2, 0, 2, 2, 2, 1, 2, 2, 2, 2, 2, 0, 3, 3, 1, 2, 2, 0, 3,\n",
       "       0, 3, 2, 2, 2, 2, 2, 3, 1, 1, 1, 2, 2, 2, 0, 2, 2, 1, 2, 0, 2, 2,\n",
       "       2, 1, 2, 2, 0, 1, 0, 1, 2, 2, 1, 2, 2, 2, 1, 3, 2, 3, 2, 2, 1, 2,\n",
       "       3, 1, 2, 1, 1, 2, 2, 1, 2, 2, 2, 0, 2, 3, 3, 3, 2, 2, 1, 3, 1, 2,\n",
       "       2, 1, 2, 2, 2, 2, 1, 2, 2, 3, 3, 2, 2, 1, 1, 2, 1, 2, 3, 1, 2, 2,\n",
       "       0, 2, 2, 1, 2, 2, 2, 1, 1, 1, 1, 3, 0, 2, 2, 2, 2, 2, 2, 1, 2, 2,\n",
       "       2, 1, 3, 3, 2, 3, 2, 3, 1, 0, 2, 3, 1, 2, 2, 0, 1, 2, 2, 2, 2, 2,\n",
       "       2, 3, 1, 1, 1, 1, 2, 3, 2, 2, 1, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0,\n",
       "       2, 2, 1, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 2, 2, 2, 3, 2, 2, 0, 1,\n",
       "       1, 1, 1, 2, 2, 1, 0, 2, 2, 2, 2, 0, 1, 2, 2, 2, 2, 1, 2, 1, 2, 0,\n",
       "       1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 3, 0, 2, 1, 2, 2])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_pred,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "for i,j in zip(np.argmax(y_test,axis = 1),np.argmax(y_pred,axis = 1)):\n",
    "    if i == j:\n",
    "        acc.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, 0, 2, 2, 3, 2, 2, 1, 2, 1, 2, 1, 0, 1, 1, 1, 0, 2, 2, 2,\n",
       "       0, 0, 2, 2, 0, 3, 0, 0, 1, 3, 2, 2, 3, 1, 1, 3, 0, 3, 2, 1, 3, 1,\n",
       "       3, 2, 3, 1, 0, 0, 0, 0, 2, 1, 0, 3, 0, 0, 1, 0, 0, 2, 2, 1, 1, 1,\n",
       "       3, 0, 2, 2, 2, 1, 2, 2, 2, 3, 0, 0, 2, 2, 3, 3, 3, 2, 2, 3, 0, 1,\n",
       "       2, 3, 2, 2, 2, 0, 1, 0, 1, 2, 1, 1, 3, 1, 3, 0, 0, 0, 0, 0, 1, 0,\n",
       "       3, 3, 3, 2, 2, 1, 1, 2, 2, 2, 1, 0, 1, 0, 3, 2, 3, 2, 0, 2, 1, 1,\n",
       "       2, 1, 0, 1, 1, 1, 0, 1, 2, 2, 0, 0, 2, 2, 2, 3, 3, 0, 2, 1, 1, 0,\n",
       "       0, 1, 0, 2, 2, 3, 0, 2, 2, 2, 1, 0, 1, 3, 0, 1, 2, 2, 3, 1, 3, 2,\n",
       "       1, 2, 2, 2, 1, 2, 2, 1, 1, 3, 1, 3, 0, 0, 2, 2, 2, 2, 0, 1, 2, 2,\n",
       "       2, 1, 0, 3, 2, 2, 0, 3, 1, 1, 2, 1, 3, 0, 3, 3, 3, 1, 2, 0, 2, 0,\n",
       "       1, 0, 0, 2, 3, 0, 1, 3, 2, 1, 1, 0, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2,\n",
       "       0, 3, 1, 3, 3, 0, 1, 1, 1, 2, 3, 2, 3, 2, 2, 0, 2, 3, 2, 2, 3, 1,\n",
       "       2, 1, 2, 0, 3, 2, 2, 3, 3, 2, 2, 1, 1, 0, 2, 3, 3, 3, 2, 0, 1, 3,\n",
       "       2, 3, 1, 3, 2, 3, 1, 0, 1, 2, 2, 0, 0, 2, 2, 2])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_test,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.423841059602649"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(acc)/len(np.argmax(y_test,axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
