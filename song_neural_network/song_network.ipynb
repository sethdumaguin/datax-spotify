{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv('songz.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we have mapped 0 to relaxed, 1 to angry, 2 to happy, and 3 to sad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(train, test):\n",
    "\n",
    "    mean_train = np.mean(train, axis=0)\n",
    "    std_train = np.std(train, axis=0)+0.000001\n",
    "    mean_test = np.mean(test, axis=0)\n",
    "    std_test = np.std(test, axis=0)+0.000001\n",
    "    X_train = (train - mean_train) / std_train\n",
    "    X_test = (test - mean_test) /std_test\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X = d.loc[:, ['title','artist','val','dB','bpm', 'nrgy', 'dnce']]\n",
    "y = d[['mood']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "le = LabelEncoder()\n",
    "X.loc[:,'title'] = le.fit_transform(X.loc[:,'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "le2 = LabelEncoder()\n",
    "X.loc[:,'artist'] = le2.fit_transform(X.loc[:,'artist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X['title'] =([int(a) for a in ['title']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "X, y = shuffle(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "Using TensorFlow backend.\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "X_test = X[1200:]\n",
    "y_test = y[1200:]\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "y_test = scaler.fit_transform(y_test)\n",
    "# X_test,_ = standardize(X_test, [0])\n",
    "# y_test, _ = standardize(y_test, [0])\n",
    "from keras.utils import np_utils\n",
    "from keras.utils import normalize\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_test)\n",
    "encoded_Y = encoder.transform(y_test)\n",
    "y_test = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[0:900]\n",
    "Y_train = y[0:900]\n",
    "X_val = X[900:1200]\n",
    "Y_val = y[900:1200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "# X_train, X_val = standardize(X_train, X_val)\n",
    "# Y_train, Y_val = standardize(Y_train,Y_val)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.fit_transform(X_val)\n",
    "Y_train= scaler.fit_transform(Y_train)\n",
    "Y_val = scaler.fit_transform(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.utils import normalize\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y_train)\n",
    "encoded_Y = encoder.transform(Y_train)\n",
    "\n",
    "Y_train =  np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.utils import normalize\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y_val)\n",
    "encoded_Y = encoder.transform(Y_val)\n",
    "Y_val = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 48)                384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 48)                192       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 24)                1176      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 24)                96        \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 24)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 12)                300       \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 12)                48        \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 78        \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 6)                 24        \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 4)                 28        \n",
      "=================================================================\n",
      "Total params: 2,326\n",
      "Trainable params: 2,146\n",
      "Non-trainable params: 180\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import BatchNormalization\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# model.add(Dense(64, input_shape=(7,), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(LeakyReLU(alpha=0.001))\n",
    "# model.add(Dropout(0.4))\n",
    "model.add(Dense(48, activation='relu', input_shape=(7,)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(24))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(12))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(6))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# #model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(Dense(4, activation='sigmoid', name='output'))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 900 samples, validate on 300 samples\n",
      "Epoch 1/50\n",
      " - 2s - loss: 0.8756 - acc: 0.4889 - val_loss: 0.7349 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.52000, saving model to best_model.h5\n",
      "Epoch 2/50\n",
      " - 0s - loss: 0.8237 - acc: 0.5106 - val_loss: 0.7266 - val_acc: 0.5233\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.52000 to 0.52333, saving model to best_model.h5\n",
      "Epoch 3/50\n",
      " - 0s - loss: 0.8236 - acc: 0.5094 - val_loss: 0.7234 - val_acc: 0.5158\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.52333\n",
      "Epoch 4/50\n",
      " - 0s - loss: 0.8040 - acc: 0.5122 - val_loss: 0.7154 - val_acc: 0.5175\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.52333\n",
      "Epoch 5/50\n",
      " - 0s - loss: 0.7810 - acc: 0.5300 - val_loss: 0.7027 - val_acc: 0.5350\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.52333 to 0.53500, saving model to best_model.h5\n",
      "Epoch 6/50\n",
      " - 0s - loss: 0.7647 - acc: 0.5314 - val_loss: 0.6939 - val_acc: 0.5500\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.53500 to 0.55000, saving model to best_model.h5\n",
      "Epoch 7/50\n",
      " - 0s - loss: 0.7545 - acc: 0.5406 - val_loss: 0.6842 - val_acc: 0.5558\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.55000 to 0.55583, saving model to best_model.h5\n",
      "Epoch 8/50\n",
      " - 0s - loss: 0.7552 - acc: 0.5486 - val_loss: 0.6774 - val_acc: 0.5650\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.55583 to 0.56500, saving model to best_model.h5\n",
      "Epoch 9/50\n",
      " - 0s - loss: 0.7354 - acc: 0.5553 - val_loss: 0.6701 - val_acc: 0.5825\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.56500 to 0.58250, saving model to best_model.h5\n",
      "Epoch 10/50\n",
      " - 0s - loss: 0.7308 - acc: 0.5531 - val_loss: 0.6622 - val_acc: 0.6042\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.58250 to 0.60417, saving model to best_model.h5\n",
      "Epoch 11/50\n",
      " - 0s - loss: 0.7162 - acc: 0.5569 - val_loss: 0.6541 - val_acc: 0.6158\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.60417 to 0.61583, saving model to best_model.h5\n",
      "Epoch 12/50\n",
      " - 0s - loss: 0.7157 - acc: 0.5714 - val_loss: 0.6489 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.61583 to 0.62500, saving model to best_model.h5\n",
      "Epoch 13/50\n",
      " - 0s - loss: 0.7127 - acc: 0.5758 - val_loss: 0.6458 - val_acc: 0.6267\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.62500 to 0.62667, saving model to best_model.h5\n",
      "Epoch 14/50\n",
      " - 0s - loss: 0.7036 - acc: 0.5719 - val_loss: 0.6421 - val_acc: 0.6333\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.62667 to 0.63333, saving model to best_model.h5\n",
      "Epoch 15/50\n",
      " - 0s - loss: 0.6929 - acc: 0.5908 - val_loss: 0.6379 - val_acc: 0.6450\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.63333 to 0.64500, saving model to best_model.h5\n",
      "Epoch 16/50\n",
      " - 0s - loss: 0.6900 - acc: 0.5908 - val_loss: 0.6335 - val_acc: 0.6517\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.64500 to 0.65167, saving model to best_model.h5\n",
      "Epoch 17/50\n",
      " - 0s - loss: 0.6695 - acc: 0.6058 - val_loss: 0.6298 - val_acc: 0.6592\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.65167 to 0.65917, saving model to best_model.h5\n",
      "Epoch 18/50\n",
      " - 0s - loss: 0.6649 - acc: 0.6153 - val_loss: 0.6257 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.65917 to 0.66583, saving model to best_model.h5\n",
      "Epoch 19/50\n",
      " - 0s - loss: 0.6696 - acc: 0.6200 - val_loss: 0.6227 - val_acc: 0.6675\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.66583 to 0.66750, saving model to best_model.h5\n",
      "Epoch 20/50\n",
      " - 0s - loss: 0.6754 - acc: 0.6044 - val_loss: 0.6200 - val_acc: 0.6725\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.66750 to 0.67250, saving model to best_model.h5\n",
      "Epoch 21/50\n",
      " - 0s - loss: 0.6623 - acc: 0.6153 - val_loss: 0.6173 - val_acc: 0.6858\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.67250 to 0.68583, saving model to best_model.h5\n",
      "Epoch 22/50\n",
      " - 0s - loss: 0.6635 - acc: 0.6144 - val_loss: 0.6144 - val_acc: 0.6942\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.68583 to 0.69417, saving model to best_model.h5\n",
      "Epoch 23/50\n",
      " - 0s - loss: 0.6558 - acc: 0.6292 - val_loss: 0.6109 - val_acc: 0.7058\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.69417 to 0.70583, saving model to best_model.h5\n",
      "Epoch 24/50\n",
      " - 0s - loss: 0.6546 - acc: 0.6317 - val_loss: 0.6080 - val_acc: 0.7092\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.70583 to 0.70917, saving model to best_model.h5\n",
      "Epoch 25/50\n",
      " - 0s - loss: 0.6460 - acc: 0.6372 - val_loss: 0.6053 - val_acc: 0.7100\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.70917 to 0.71000, saving model to best_model.h5\n",
      "Epoch 26/50\n",
      " - 0s - loss: 0.6499 - acc: 0.6406 - val_loss: 0.6023 - val_acc: 0.7175\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.71000 to 0.71750, saving model to best_model.h5\n",
      "Epoch 27/50\n",
      " - 0s - loss: 0.6434 - acc: 0.6419 - val_loss: 0.5995 - val_acc: 0.7192\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.71750 to 0.71917, saving model to best_model.h5\n",
      "Epoch 28/50\n",
      " - 0s - loss: 0.6365 - acc: 0.6433 - val_loss: 0.5967 - val_acc: 0.7225\n",
      "\n",
      "Epoch 00028: val_acc improved from 0.71917 to 0.72250, saving model to best_model.h5\n",
      "Epoch 29/50\n",
      " - 0s - loss: 0.6410 - acc: 0.6514 - val_loss: 0.5934 - val_acc: 0.7250\n",
      "\n",
      "Epoch 00029: val_acc improved from 0.72250 to 0.72500, saving model to best_model.h5\n",
      "Epoch 30/50\n",
      " - 0s - loss: 0.6375 - acc: 0.6481 - val_loss: 0.5915 - val_acc: 0.7258\n",
      "\n",
      "Epoch 00030: val_acc improved from 0.72500 to 0.72583, saving model to best_model.h5\n",
      "Epoch 31/50\n",
      " - 0s - loss: 0.6373 - acc: 0.6422 - val_loss: 0.5903 - val_acc: 0.7167\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.72583\n",
      "Epoch 32/50\n",
      " - 0s - loss: 0.6255 - acc: 0.6658 - val_loss: 0.5893 - val_acc: 0.7200\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.72583\n",
      "Epoch 33/50\n",
      " - 0s - loss: 0.6258 - acc: 0.6744 - val_loss: 0.5874 - val_acc: 0.7208\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.72583\n",
      "Epoch 34/50\n",
      " - 0s - loss: 0.6248 - acc: 0.6675 - val_loss: 0.5854 - val_acc: 0.7233\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.72583\n",
      "Epoch 35/50\n",
      " - 0s - loss: 0.6228 - acc: 0.6606 - val_loss: 0.5838 - val_acc: 0.7258\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.72583\n",
      "Epoch 36/50\n",
      " - 0s - loss: 0.6265 - acc: 0.6717 - val_loss: 0.5818 - val_acc: 0.7317\n",
      "\n",
      "Epoch 00036: val_acc improved from 0.72583 to 0.73167, saving model to best_model.h5\n",
      "Epoch 37/50\n",
      " - 0s - loss: 0.6151 - acc: 0.6758 - val_loss: 0.5796 - val_acc: 0.7342\n",
      "\n",
      "Epoch 00037: val_acc improved from 0.73167 to 0.73417, saving model to best_model.h5\n",
      "Epoch 38/50\n",
      " - 0s - loss: 0.6128 - acc: 0.6786 - val_loss: 0.5785 - val_acc: 0.7392\n",
      "\n",
      "Epoch 00038: val_acc improved from 0.73417 to 0.73917, saving model to best_model.h5\n",
      "Epoch 39/50\n",
      " - 0s - loss: 0.6194 - acc: 0.6794 - val_loss: 0.5769 - val_acc: 0.7367\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.73917\n",
      "Epoch 40/50\n",
      " - 0s - loss: 0.6169 - acc: 0.6839 - val_loss: 0.5750 - val_acc: 0.7392\n",
      "\n",
      "Epoch 00040: val_acc improved from 0.73917 to 0.73917, saving model to best_model.h5\n",
      "Epoch 41/50\n",
      " - 0s - loss: 0.6023 - acc: 0.6939 - val_loss: 0.5732 - val_acc: 0.7367\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.73917\n",
      "Epoch 42/50\n",
      " - 0s - loss: 0.6138 - acc: 0.6769 - val_loss: 0.5717 - val_acc: 0.7408\n",
      "\n",
      "Epoch 00042: val_acc improved from 0.73917 to 0.74083, saving model to best_model.h5\n",
      "Epoch 43/50\n",
      " - 0s - loss: 0.5985 - acc: 0.7042 - val_loss: 0.5702 - val_acc: 0.7425\n",
      "\n",
      "Epoch 00043: val_acc improved from 0.74083 to 0.74250, saving model to best_model.h5\n",
      "Epoch 44/50\n",
      " - 0s - loss: 0.6059 - acc: 0.6886 - val_loss: 0.5685 - val_acc: 0.7433\n",
      "\n",
      "Epoch 00044: val_acc improved from 0.74250 to 0.74333, saving model to best_model.h5\n",
      "Epoch 45/50\n",
      " - 0s - loss: 0.6065 - acc: 0.6967 - val_loss: 0.5669 - val_acc: 0.7417\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.74333\n",
      "Epoch 46/50\n",
      " - 0s - loss: 0.6050 - acc: 0.6967 - val_loss: 0.5654 - val_acc: 0.7408\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.74333\n",
      "Epoch 47/50\n",
      " - 0s - loss: 0.6014 - acc: 0.7039 - val_loss: 0.5641 - val_acc: 0.7408\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.74333\n",
      "Epoch 48/50\n",
      " - 0s - loss: 0.5926 - acc: 0.7022 - val_loss: 0.5629 - val_acc: 0.7433\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.74333\n",
      "Epoch 49/50\n",
      " - 0s - loss: 0.5934 - acc: 0.7022 - val_loss: 0.5619 - val_acc: 0.7450\n",
      "\n",
      "Epoch 00049: val_acc improved from 0.74333 to 0.74500, saving model to best_model.h5\n",
      "Epoch 50/50\n",
      " - 0s - loss: 0.5956 - acc: 0.7078 - val_loss: 0.5612 - val_acc: 0.7475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00050: val_acc improved from 0.74500 to 0.74750, saving model to best_model.h5\n"
     ]
    }
   ],
   "source": [
    "# Adam optimizer with learning rate of 0.001\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "optimizer = Adam(lr=0.001)\n",
    "adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-10, decay=0)\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(optimizer = 'adam' , loss='binary_crossentropy', metrics=['accuracy'])\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "hist = model.fit(X_train, Y_train, verbose=2, batch_size=128, epochs=50, validation_data=(X_val,Y_val),callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5645008505574914, 0.7483443700714617]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(1, figsize=(10,7))\n",
    "\n",
    "# Training accuracy\n",
    "plt.subplot(221)\n",
    "plt.plot(range(0,50), hist.history['acc'], c = 'forestgreen');\n",
    "plt.title('Training accuracy per epoch');\n",
    "\n",
    "# Training loss #\n",
    "plt.subplot(222)\n",
    "plt.plot(range(0,50), hist.history['loss'], c = 'forestgreen');\n",
    "plt.title('Training loss per epoch');\n",
    "\n",
    "# Validation accuracy #\n",
    "plt.subplot(223)\n",
    "plt.plot(range(0,50), hist.history['val_acc'], c = 'blue');\n",
    "plt.title('Validation accuracy per epoch');\n",
    "\n",
    "# Validation loss #\n",
    "plt.subplot(224)\n",
    "plt.plot(range(0,50), hist.history['val_loss'], c = 'blue');\n",
    "plt.title('Validation loss per epoch');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'song_NN.pickle'\n",
    "\n",
    "pickle_out = open(filename, 'wb')\n",
    "\n",
    "pickle.dump(model, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"song_NN.pickle\",\"rb\")\n",
    "example_dict = pickle.load(pickle_in)\n",
    "y_pred = example_dict.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = []\n",
    "for i in y_pred:\n",
    "    yvals = np.where(i == np.amax(i))\n",
    "    arr.append(yvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = []\n",
    "for i in arr:\n",
    "    array.append(i[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_arr = np.array(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0\n",
    "for i in y[1200:].values:\n",
    "    for j in fin_arr:\n",
    "        if i == j:\n",
    "            acc += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89.83112582781457"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
